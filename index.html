---
layout: default
---
<!-- Stitched together from pieces of the websites of two Berkeley students: Meena Jagadeesan and Alex Wei --!>

<!-- Headshot -->
<div class="wrapper">
<header id="home-header">
  <img src="/assets/gustaf.jpg" id="headshot" class="rounded" alt="Headshot" width="250" height="250" />
  <h1 id="home-title">Gustaf Ahdritz</h1>
  <address id="address"><!--
    --><div><span class="addri"></span></div><!--
    --><span class="addrj"></span><!--
    --><div><span class="addrk"></span></div>
  </address>
</header>

<div class="text">
    <span> I'm a third-year PhD student in Computer Science at Harvard University. I'm a member of the <a href="https://mlfoundations.org">Machine Learning Foundations Group</a> and am advised by  <a href="https://boazbarak.org">Boaz&nbsp;Barak</a> and <a href="http://www.jfrankle.com">Jonathan&nbsp;Frankle</a>. I'm supported by a fellowship from Harvard's <a href="https://www.harvard.edu/kempner-institute/">Kempner Institute</a>.</span><br><br>
    <span> I'm broadly interested in empirical investigations of the properties of realistic deep neural networks. At the moment, I'm thinking about uncertainty in large language models.</span><br><br>
    <span> I graduated from Columbia with a B.A. in Computer Science &amp; History (2020) and an M.S. in Computer Science (2021). There, I worked with <a href="https://aqlab.io">Mohammed AlQuraishi</a> on the applied task of protein structure prediction and lead the development of <a href="https://www.github.com/aqlaboratory/openfold">OpenFold</a>. I also spent time in <a href="http://www.cs.columbia.edu/~kathy/">Kathleen McKeown</a>'s lab and the <a href="https://history-lab.org">History Lab</a>.</span><br><br>
    <span> Here's my <a href=/assets/cv.pdf>CV</a>.</span><br>
    
    <div id="paper-header">
        <a id="papers"></a>
        <h3>Papers</h3>
        <span id="eq-note">* denotes equal contribution</span>
    </div>

    <h4><span>Preprints</span></h4>
    <ul style="list-style-type:square">
        <li style="margin-bottom:10px"><b><span>Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts</span></b>
            <br>
            <span class="ast"></span>Garrett&nbsp;Tanzer, 
            <span class="ast"></span><b>Gustaf&nbsp;Ahdritz</b>, 
            Luke&nbsp;Melas-Kyriazi,
            <br>
            <span><em>arXiv</em>, 2024.</span>
            <br> 
            [<a href="https://arxiv.org/abs/2405.13203">paper</a>]
            [<a href="https://github.com/gahdritz/rtic">code</a>]
            [<a href="/bibtex/rtic.bib">bibtex</a>]
            <br>
            <br>
            <em>TL;DR: Training language models directly on timed, diarized transcripts (e.g. instant messenger logs) permits true real-time interactivity.</em>
    </ul>

    <h4><span>Workshop papers</span></h4>
    <ul style="list-style-type:square">
        <li style="margin-bottom:10px"><b><span>Soft prompting might be a bug, not a feature</span></b>
            <br>
            <span class="ast"></span>Luke&nbsp;Bailey, 
            <span class="ast"></span><b>Gustaf&nbsp;Ahdritz</b>,
            <span class="ast"></span>Anat&nbsp;Kleiman, 
            Siddharth&nbsp;Swaroop,
            Finale&nbsp;Doshi-Velez,
            Weiwei&nbsp;Pan
            <br>
            <span><em>Workshop on Challenges in Deployable Generative AI</em>, ICML 2023.</span>
            <br> 
            [<a href="https://openreview.net/forum?id=MHWDdMEJ5s">paper</a>]
            [<a href="/bibtex/soft_prompts.bib">bibtex</a>]
            <br>
            <br>
            <em>TL;DR: Contrary to prior speculation, we find that soft prompts (created with "prompt-" or "prefix-tuning") differ from natural token embeddings in key ways, complicating attempts to decode them back into natural language.</em>
    </ul>

    <h4><span>Publications</span></h4>
    <ul style="list-style-type:square">
        <li style="margin-bottom:10px"><b><span>Distinguishing the Knowable from the Unknowable with Language Models</span></b>
            <br>
            <span class="ast"></span><b>Gustaf&nbsp;Ahdritz</b>, 
            <span class="ast"></span>Tian&nbsp;Qin, 
            Nikhil&nbsp;Vyas,
            Boaz&nbsp;Barak,
            Benjamin&nbsp;L.&nbsp;Edelman
            <br>
            <span><em>ICML</em>, 2024.</span>
            <br> 
            [<a href="https://icml.cc/virtual/2024/poster/32819">paper</a>]
            [<a href="https://github.com/gahdritz/llm_uncertainty">code</a>]
            [<a href="https://kempnerinstitute.harvard.edu/research/deeper-learning/distinguishing-the-knowable-from-the-unknowable-with-language-models/">blog</a>]
            [<a href="https://x.com/gahdritz/status/1780618634832036174">tweetorial</a>]
            [<a href="/bibtex/distinguishing.bib">bibtex</a>]
            <br>
            <br>
            <em>TL;DR: Linear probes of language model activations can predict when the predictive entropy of much larger and more knowledgeable models is close to zero, and they even work out-of-distribution!</em>
        <li style="margin-bottom:10px"><b><span>OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization</span></b>
            <br>
            <span class="ast"></span><b>Gustaf&nbsp;Ahdritz</b>, 
            <span class="ast"></span>Nazim&nbsp;Bouatta, 
            Christina&nbsp;Floristean,
            Sachin&nbsp;Kadyan,
            Qinghui&nbsp;Xia,
            William&nbsp;Gerecke,
            Timothy&nbsp;J.&nbsp;O'Donnell,
            Daniel&nbsp;Berenberg,
            Ian&nbsp;Fisk,
            Niccol√≤&nbsp;Zanichelli,
            Bo&nbsp;Zhang,
            Arkadiusz&nbsp;Nowaczynski,
            Bei&nbsp;Wang,
            Marta&nbsp;M.&nbsp;Stepniewska-Dziubinska,
            Shang&nbsp;Zhang,
            Adegoke&nbsp;Ojewole,
            Murat&nbsp;Efe&nbsp;Guney,
            Stella&nbsp;Biderman,
            Andrew&nbsp;M.&nbsp;Watkins,
            Stephen&nbsp;Ra,
            Pablo&nbsp;Ribalta&nbsp;Lorenzo,
            Lucas&nbsp;Nivon,
            Brian&nbsp;Weitzner,
            Yih-En&nbsp;Andrew&nbsp;Ban,
            Shiyang&nbsp;Chen,
            Minjia&nbsp;Zhang,
            Conglong&nbsp;Li,
            Shuaiwen&nbsp;Leon&nbsp;Song,
            Yuxiong&nbsp;He,
            Peter&nbsp;K.&nbsp;Sorger,
            Emad&nbsp;Mostaque,
            Zhao&nbsp;Zhang,
            Richard&nbsp;Bonneau,
            Mohammed&nbsp;AlQuraishi 
            <br>
            <span><em>Nature Methods</em> 21, 1514-1524 (2024).</span>
            <br> 
            [<a href="https://www.nature.com/articles/s41592-024-02272-z">paper</a>]
            [<a href="https://www.github.com/aqlaboratory/openfold">code</a>]
            [<a href="https://www.youtube.com/watch?v=W92xVnUMkU0">talk</a>]
            [<a href="https://www.nature.com/articles/s41587-024-02290-4">coverage</a>]
            [<a href="/bibtex/openfold.bib">bibtex</a>]
            <br>
            <br>
            <em>TL;DR: We created the first trainable, open-source reproduction of AlphaFold2 and used it to study how the model learns to fold. We found surprisingly fast convergence, robustness to lack of diversity in the training set, and regular progressions in the dimensionality of predicted structures over the course of training. We also optimized the model, making inference possible on much longer sequences, and added new features to improve training stability.</em>
        <li style="margin-bottom:10px"><b><span>OpenProteinSet: Training data for structural biology at scale</span></b>
            <br>
            <b>Gustaf&nbsp;Ahdritz</b>, 
            Nazim&nbsp;Bouatta, 
            Sachin&nbsp;Kadyan,
            Lukas&nbsp;Jarosch,
            Daniel&nbsp;Berenberg,
            Ian&nbsp;Fisk,
            Andrew&nbsp;M.&nbsp;Watkins,
            Stephen&nbsp;Ra,
            Richard&nbsp;Bonneau,
            Mohammed&nbsp;AlQuraishi 
            <br>
            <span><em>NeurIPS 2023 Track on Datasets and Benchmarks</em>, 2023.</span>
            <br> 
            [<a href="https://neurips.cc/virtual/2023/poster/73507">paper</a>]
            [<a href="https://registry.opendata.aws/openfold/">data</a>]
            [<a href="https://arxiv.org/abs/2308.05326">preprint</a>]
            [<a href="/bibtex/openproteinset.bib">bibtex</a>]
            <br>
            <br>
            <em>TL;DR: We present the largest open repository of precomputed multiple sequence alignments (MSAs) of proteins, representing millions of compute hours. MSAs are important primitives across bioinformatics, but their steep computational cost has previously limited their accessibility outside large research labs in industry (notably DeepMind and Meta, of AlphaFold2 and the MSA Transformer, respectively).</em>
        <li style="margin-bottom:10px"><b><span>Single-sequence protein structure prediction using a language model and deep learning</span></b>
            <br>
            <span class="ast"></span>Ratul&nbsp;Chowdhury, 
            <span class="ast"></span>Nazim&nbsp;Bouatta, 
            <span class="ast"></span>Surojit&nbsp;Biswas, 
            <span class="ast"></span>Christina&nbsp;Floristean, 
            Anant&nbsp;Kharkar, 
            Koushik&nbsp;Roy, 
            Charlotte&nbsp;Rochereau, 
            <b>Gustaf&nbsp;Ahdritz</b>, 
            Joanna&nbsp;Zhang, 
            George&nbsp;M.&nbsp;Church, 
            Peter&nbsp;K.&nbsp;Sorger, 
            Mohammed&nbsp;AlQuraishi 
            <br>
            <span><em>Nature Biotechnology</em>, 2022.</span>
            <br> 
            [<a href="https://www.nature.com/articles/s41587-022-01432-w">paper</a>]
            [<a href="https://www.github.com/aqlaboratory/rgn2">code</a>]
            [<a href="https://www.nature.com/articles/s41587-022-01466-0">coverage</a>]
            [<a href="/bibtex/rgn2.bib">bibtex</a>]
            <br>
            <br>
            <em>TL;DR: We present RGN2, an end-to-end "single-sequence" protein structure prediction model that relies on a small protein language model rather than multiple sequence alignments. RGN2 outperforms AlphaFold2 and RoseTTAFold on orphan proteins and is faster by orders of magnitude.</em>
    </ul>
    <a id="teaching"></a>
    <h3><span>Teaching</span></h3>

    I've served as a teaching fellow/assistant for the following courses at Harvard and Columbia:
    <ul style="list-style-type:square">
        <li style="margin-bottom:10px"><em>Spring 2023</em>: Foundations of Deep Learning (Harvard COMPSCI 229br) with Boaz Barak
        <li style="margin-bottom:10px"><em>Spring 2019 - Spring 2021</em>: Advanced Programming (Columbia COMS 3157) with Jae Woo Lee
    </ul>
    <a id="awards"></a>
    <h3><span>Awards & Fellowships</span></h3>
    <ul style="list-style-type:square">
        <li style="margin-bottom:10px"><em>2023 - 2028</em>: <a href="https://www.harvard.edu/kempner-institute/2023/05/31/kempner-institute-announces-recipients-of-inaugural-graduate-student-fellowships/">Kempner Institute Graduate Fellowship</a> (Kempner Institute, Harvard)</li>
        <li style="margin-bottom:10px"><em>2022 - 2028</em>: <a href="https://ashfordfellows.fas.harvard.edu/about">Ashford Fellowship</a> (Harvard)</li>
        <li style="margin-bottom:10px"><em>2020</em>: Andrew P. Kosoresow Memorial Award for Excellence in Teaching and Service (Columbia SEAS)</li>
        <li style="margin-bottom:10px"><em>2019</em>: Dean Hawkes Memorial Prize (Columbia College)</li>
    </ul>
    <h3><a id="links"></a><span>Links</span></h3>
        [<a href="https://github.com/gahdritz">github</a>]
        [<a href="https://scholar.google.com/citations?user=oCO_ClkAAAAJ&hl=en&oi=ao">google scholar</a>]
        [<a href="https://twitter.com/gahdritz">twitter</a>]  
    <br><br><br>
</div>
</div>
